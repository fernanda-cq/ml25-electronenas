{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Métodos de agrupamiento\n",
    "\n",
    "En este ejercicio, aplicaremos los conceptos vistos en clase referentes a métodos de agrupamiento no supervisados. Específicamente, explorarás el uso de KMeans y DBScan usando la librería de scikit-learn. En la primera sección analizaremos K-means y en la segunda DBScan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 1. K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.cluster import DBSCAN, KMeans\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "random_state = 0 # Fijamos la semilla aleatoria para que siempre den los mismos datos\n",
    "np.random.seed(random_state)\n",
    "# Podemos establecer los colores con los que se graficará con matplotlib\n",
    "# utilizando un cmap distinto\n",
    "matplotlib.rc('image', cmap='nipy_spectral')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'datasets' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Generaremos 3 blobs, cada uno con datos iguales. En total habran 1000 puntos distribuidos en estos blobs.\u001b[39;00m\n\u001b[32m      2\u001b[39m n_samples = \u001b[32m1000\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m X, _ = \u001b[43mdatasets\u001b[49m.make_blobs(n_samples=n_samples, centers=\u001b[32m3\u001b[39m, random_state=random_state)\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Utilizaremos el algoritmo de KMeans para encontrar clusters en los datos\u001b[39;00m\n\u001b[32m      6\u001b[39m n_cols = \u001b[32m4\u001b[39m\n",
      "\u001b[31mNameError\u001b[39m: name 'datasets' is not defined"
     ]
    }
   ],
   "source": [
    "# Generaremos 3 blobs, cada uno con datos iguales. En total habran 1000 puntos distribuidos en estos blobs.\n",
    "n_samples = 1000\n",
    "X, _ = datasets.make_blobs(n_samples=n_samples, centers=3, random_state=random_state)\n",
    "\n",
    "# Utilizaremos el algoritmo de KMeans para encontrar clusters en los datos\n",
    "n_cols = 4\n",
    "n_clusters_list = [2, 3, 4, 5]\n",
    "rows = len(n_clusters_list) // n_cols\n",
    "fig, axes = plt.subplots(rows, n_cols, figsize=(4*n_cols, 4))\n",
    "axes = axes.flatten()\n",
    "for ax, n_clusters in zip(axes, n_clusters_list):\n",
    "    # TODO: Encuentra los clusters utilizando el algoritmo de KMeans implementado en scikit-learn\n",
    "    # ====== Start of solution =====\n",
    "    model = KMeans(n_clusters=n_clusters, random_state=random_state)\n",
    "    model.fit(X)\n",
    "    # TODO: Grafica los datos usando scatter, recuerda que X = (x, y)\n",
    "ax.scatter(X[:, 0], x[:, 1], c=model.labels_,s=30, cmap=\"viridis\", alpha=0.6)\n",
    "    # TODO: Lee la documentación de sklearn para obtener los centroides del modelo entrenado\n",
    "    # y grafícalos con ax.scatter\n",
    "centers=model.cluster_centers_\n",
    "ax.scatter(centers[:, 0], centers[:, 1], c=\"red\", s=200, marker=\"X\")\n",
    "\n",
    "ax.set_title(f\"K-means: {n_clusters} clusters\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la sección anterior utilizamos 2, 3, 4 y 5 clusters para encontar patrones en los datos y los resultados se graficaron.\n",
    "\n",
    "TODO: Contesta la siguiente pregunta\n",
    "\n",
    "- ¿Cúal propuesta de clusters consideras la mejor y porqué?\n",
    "La de 3 clusters, ya que separa cada grupo y los centroiden se ubican en el centro de cada punto. Ademas que no se pierde informacion y no se divide tantas veces."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Definiendo métricas de evaluación"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el algoritmo de K-means debemos seleccionar la cantidad de clusters para poder hacer fit al modelo. Para decidir cuantos clusters utilizar podemos evaluar visualmente o hacer un análisis cuantitativo. \n",
    "\n",
    "Una estrategia común para realizar el análisis consiste en entrenar modelos con multiples propuestas de k-grupos y seleccionar la propuesta con el mejor rendimiento. Para ello se debe utilizar una métrica que evalúe el desempeño del algoritmo.\n",
    "\n",
    "El [\"silhouette score\"](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html) es una métrica muy útil para este objetivo que consiste en medir la distancia intra-grupo e inter-grupo. Esta métrica considera como buen grupo a aquel que tiene una pequeña distancia promedio intra-grupo y que se encuentra muy separado de los demas grupos. Un acomodo con buenos grupos tendrá un silhoute score cercano a 1 y una mala selección de grupos tendrá un score cercano -1. \n",
    "\n",
    "En la siguiente sección seguiremos esta estrategia para determinar la mejor cantidad de grupos en los datos provistos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "Ks = range(2,6)\n",
    "scores = []\n",
    "for k in Ks:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=0).fit(X)\n",
    "    # TODO: Obten el silhouete score de nuestro modelo\n",
    "    # ====== Start of solution =====\n",
    "    score = silhouette_score(X, kmeans.labels_) #evalua que tan buenos son los clusters\n",
    "    # ====== End of solution =====\n",
    "    scores.append(score)\n",
    "\n",
    "plt.plot(Ks,scores,'-o', markersize=8)\n",
    "plt.xlabel(\"valor de K\")\n",
    "plt.ylabel(\"Silhouette Score\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Contesta la siguiente pregunta\n",
    "\n",
    "De acuerdo al silhouete score\n",
    "- ¿Qué sería mejor, elegir 4 o 5 clusters?\n",
    "4 clusters porque si son mas clusters, el silhouete score disminute.\n",
    "- ¿Cuántos clusters se deberían utilizar para entrenar el algoritmo? \n",
    "3 clusters\n",
    "¿Concuerda con tu selección realizada tras visualizar los datos?\n",
    "Si"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Rompiendo las asunciones de K-Means"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-means es un algoritmo que funciona en base a algunas asunciones que pueden no siempre ser ciertas, en esta seccion analizaremos algunos casos en los cuales el K-means puede que no genere la solución esperada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples=200\n",
    "# =============================================================================\n",
    "# Generamos diferentes conjuntos de datos que rompen las asunciones de K-Means\n",
    "# =============================================================================\n",
    "noisy_circles = datasets.make_circles(n_samples=n_samples, factor=0.5, noise=0.05)\n",
    "noisy_moons = datasets.make_moons(n_samples=n_samples, noise=0.05)\n",
    "blobs = datasets.make_blobs(n_samples=n_samples, random_state=8)\n",
    "# Anisotropicly distributed data\n",
    "random_state = 170\n",
    "X, y = datasets.make_blobs(n_samples=n_samples, random_state=random_state)\n",
    "transformation = [[0.6, -0.6], [-0.4, 0.8]]\n",
    "X_aniso = np.dot(X, transformation)\n",
    "aniso = (X_aniso, y)\n",
    "# Blobs con varianzas variadas\n",
    "varied = datasets.make_blobs(\n",
    "    n_samples=n_samples, cluster_std=[1.0, 2.5, 0.5], random_state=random_state\n",
    ")\n",
    "\n",
    "# Cada dataset es un tuple (datos, etiquetas)\n",
    "datasets = {\"Círculos\": noisy_circles,\n",
    "            \"Lunas\": noisy_moons, \n",
    "            \"Blobs\": blobs, \n",
    "            \"Varianzas distintas\": varied,\n",
    "            # \"Aleatorio\": no_structure,\n",
    "            \"Datos anisotrópicos\": aniso}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============\n",
    "# Graficamos los datos\n",
    "# ============\n",
    "n_rows = 1\n",
    "n_cols = len(datasets)\n",
    "fig, axs = plt.subplots(n_rows, n_cols, figsize=(3*n_cols, 2))\n",
    "axs = axs.flatten()\n",
    "for (name, data), ax in zip(datasets.items(), axs):\n",
    "    X, _ = data\n",
    "    # Normalizamos los datos para que todos estén en la misma escala\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "    # TODO: Grafica los datos usando scatter\n",
    "    # ====== Start of solution =====\n",
    "    ax.scatter(X[:, 0], X[:, 1], s=20, alpha=0.7)\n",
    "    # ====== End of solution =====\n",
    "    ax.set_title(name)    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Para los conjuntos de datos anteriores...\n",
    "Aplica K-means a cada uno de los datasets anteriores. Determina los hiperparámetros (k) que resulten en la mejor asignación según tu criterio para cada grupo.\n",
    "- ¿En que conjunto/os esperas que k-means realice una solución adecuada?\n",
    "En los blobs\n",
    "- ¿Cuales son las asunciones de k-means?\n",
    "Clusters convexos y de tamaño, relacion lineal con la media y numero de clusters conocido\n",
    "- ¿Cual de estos conjuntos NO rompe las asunciones de k-means? Los blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_rows = 1\n",
    "n_cols = len(datasets)\n",
    "fig, axs = plt.subplots(n_rows, n_cols, figsize=(3*n_cols, 2))\n",
    "axs = axs.flatten()\n",
    "# TODO: Modifica la cantidad de clusters para cada dataset hasta que encuentres un resultado que te satisfaga\n",
    "n_clusters = {\"Círculos\": 2,\n",
    "              \"Lunas\": 2,\n",
    "              \"Blobs\": 3,\n",
    "              \"Varianzas distintas\": 3,\n",
    "              \"Anisotrópicos\": 3}\n",
    "for (name, data), n, ax in zip(datasets.items(), n_clusters.values(), axs):\n",
    "    X, _ = data\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "    ax.set_title(name)\n",
    "    # TODO: Aplica K-means para encontrar los grupos y los centros de cada cluster\n",
    "    # Después modifica los hiperparámetros en n_clusters según consideres apropiado para cada dataset\n",
    "    # ====== Start of solution =====\n",
    "    kmeans=KMeans(n_clusters=n, random_state=0).fit(X)\n",
    "    centers = kmeans.cluster_centers_\n",
    "    # ====== End of solution =====\n",
    "    ax.scatter(X[:, 0], X[:, 1], s=10, c=kmeans.labels_, vmin=-1)\n",
    "    ax.scatter(centers[:, 0], centers[:, 1], marker=\"+\", s=30, c='k')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. DBScan\n",
    "DBScan es otro método de agrupamiento el cual encuentra grupos que funciona a través de densidad local.\n",
    "En las siguientes celdas, aplica DBScan a cada uno de los datasets anteriores y compara los resultados con los de K-Means. Ajusta los hiperparámetros según tu criterio para lograr un agrupamiento adecuado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_rows = 1\n",
    "n_cols = len(datasets)\n",
    "fig, axs = plt.subplots(n_rows, n_cols, figsize=(3*n_cols, 2))\n",
    "axs = axs.flatten()\n",
    "# TODO: Modifica los hiper parámetros de dbscan hasta encontrar un resultado que te satisfaga\n",
    "params = {\"Círculos\": {\"eps\": 0.2, \"min_samples\": 5},\n",
    "          \"Lunas\": {\"eps\": 0.2, \"min_samples\": 5},\n",
    "          \"Blobs\": {\"eps\": 0.3, \"min_samples\": 5},\n",
    "          \"Varianzas distintas\": {\"eps\": 0.3, \"min_samples\": 5},\n",
    "          \"Datos anisotrópicos\": {\"eps\": 0.3, \"min_samples\": 5}}\n",
    "for ((name, data), ax) in zip(datasets.items(), axs):\n",
    "    X, _ = data\n",
    "    X = StandardScaler().fit_transform(X)\n",
    "    ax.set_title(name)\n",
    "    eps = params[name]['eps']\n",
    "    min_samples = params[name]['min_samples']\n",
    "    # TODO: Aplica DBSCAN para encontrar los clusters\n",
    "    # Después modifica los hiperparámetros en n_clusters según consideres apropiado para cada dataset\n",
    "    # ====== Start of solution =====\n",
    "    db = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "    grupos = db.fit_predict(X)\n",
    "    # ====== End of solution =====\n",
    "    print(f\"Clusteres en {name}\", np.unique(grupos))\n",
    "    ax.scatter(X[:, 0], X[:, 1], s=10, c=grupos, vmin=-1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: Contesta la siguientes preguntas\n",
    "- ¿Qué diferencia encuentras entre los clusters encontrados con DBSCAN y los clusters encontrados con K-Means?\n",
    "Los kmeans los busca de tamaño similares y los DBSCAN encuentra cualquier cluster\n",
    "- ¿Qué metodo clasifica mejor los clusters de Lunas y a qué crees que se deba?\n",
    "DBSCAN porque sigue mejor la forma de las lunas\n",
    "- ¿Qué son los puntos negros encontrados con DBSCAN?\n",
    "Se le llama ruido, se refiere a que el punto no tiene suficientes a su alrededor para pertenecer a un cluster\n",
    "- ¿En qué casos crees que sea recomendable utilizar K-means sobre DBSCAN?\n",
    "Cuando se conoce el numero de cluster y estos sean esfericos.\n",
    "- En base a tu experiencia ¿que desventajas notas para DBScan contra K-means?\n",
    "sus desventajas es que puede fallar cuando existen clusters de densidades diferentes y no predice el cluster de nuevos puntos\n",
    "- En base a tu experiencia ¿que ventajas notas para DBScan contra K-means?\n",
    "Realiza clusters de cualquier forma y no tienen que ser del mismo tamaño los clusters\n",
    "- Si entrenamos DBSCAN y queremos buscar un grupo al que pertenezca un punto nuevo, ¿Existe alguna función de sk-learn que nos permita hacer esto?¿Cómo podríamos predecir el grupo al que pertenece un nuevo datapoint? No existe una funcion. Para predecirlo se podria calcular la distancia del nuevo datapoint a los que ya existen y ver si cae dentro de algun radio."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
